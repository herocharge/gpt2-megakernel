{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962bafd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "model_name = \"gpt2\"  # smallest GPT-2\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "output = model(**inputs)\n",
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe21b6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 2304])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a4af48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "010ef777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49488,   314,   481,   407,  2740,    11,  4249, 49671,    26,  1114,\n",
       "           616,  5848,   318,  2626,   832,   220]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Tomorrow I will not speak, nor weep; For my soul is lost through \"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c00d7bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  11, 1101,  307,  307,  286,  475,  466,   11,  475,  314, 5848,  318,\n",
       "         407,   11,  262, 2171])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs).logits[0].argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557126c",
   "metadata": {},
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c78f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1101,  0.0403, -0.1275, -0.0927, -0.0506,  0.0112, -0.0839, -0.1300,\n",
       "        -0.0797, -0.0401], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wte.weight[:10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf1ce945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-0.0016, grad_fn=<SelectBackward0>),\n",
       " tensor(1.8395, grad_fn=<SelectBackward0>),\n",
       " tensor(2.1406, grad_fn=<SelectBackward0>),\n",
       " tensor(2.0247, grad_fn=<SelectBackward0>),\n",
       " tensor(2.0568, grad_fn=<SelectBackward0>),\n",
       " tensor(1.8415, grad_fn=<SelectBackward0>),\n",
       " tensor(1.8090, grad_fn=<SelectBackward0>),\n",
       " tensor(1.6873, grad_fn=<SelectBackward0>),\n",
       " tensor(1.6717, grad_fn=<SelectBackward0>),\n",
       " tensor(1.6966, grad_fn=<SelectBackward0>),\n",
       " tensor(1.5764, grad_fn=<SelectBackward0>),\n",
       " tensor(1.4559, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0812, grad_fn=<SelectBackward0>)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model(**inputs, output_hidden_states=True).hidden_states[i][0][0][0] for i in range(13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8729d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = model(**inputs, output_hidden_states=True).hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7df55862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2232, 0.1820, 0.1534, 0.1917, 0.2036, 0.1948, 0.1467, 0.1865, 0.2143,\n",
       "        0.1956, 0.2118, 0.2153, 0.1882, 0.2074, 0.1871, 0.2040, 0.2044, 0.1900,\n",
       "        0.1952, 0.0475, 0.1909, 0.2115, 0.1971, 0.2202, 0.1998, 0.2108, 0.2303,\n",
       "        0.1879, 0.1939, 0.2018, 0.1891, 0.1861, 0.1958, 0.1832, 0.1978, 0.2243,\n",
       "        0.0706, 0.1958, 0.1943, 0.1939, 0.1978, 0.1951, 0.1995, 0.1912, 0.2083,\n",
       "        0.2037, 0.1849, 0.1945, 0.2189, 0.0419, 0.1977, 0.1979, 0.0608, 0.1824,\n",
       "        0.2055, 0.0476, 0.1892, 0.2079, 0.2047, 0.2233, 0.2097, 0.2075, 0.2076,\n",
       "        0.1793, 0.1312, 0.1841, 0.1939, 0.1561, 0.0577, 0.1948, 0.2048, 0.1717,\n",
       "        0.1942, 0.1708, 0.1989, 0.1993, 0.2082, 0.1071, 0.1968, 0.1770, 0.2164,\n",
       "        0.1864, 0.1938, 0.2184, 0.1343, 0.1707, 0.0683, 0.1401, 0.1823, 0.2045,\n",
       "        0.2007, 0.1853, 0.1783, 0.1889, 0.1870, 0.1975, 0.2114, 0.2108, 0.2083,\n",
       "        0.2409, 0.1938, 0.2022, 0.0857, 0.1823, 0.1879, 0.1979, 0.1850, 0.1029,\n",
       "        0.1762, 0.1953, 0.2231, 0.2006, 0.2022, 0.2134, 0.1970, 0.1820, 0.0568,\n",
       "        0.2269, 0.1882, 0.1770, 0.1880, 0.1910, 0.1872, 0.1613, 0.1946, 0.1930,\n",
       "        0.1981, 0.2030, 0.1848, 0.2341, 0.1832, 0.1893, 0.2368, 0.2085, 0.1833,\n",
       "        0.2083, 0.2009, 0.2212, 0.1342, 0.0614, 0.1913, 0.1812, 0.1041, 0.1957,\n",
       "        0.1902, 0.1355, 0.2145, 0.1974, 0.1904, 0.1997, 0.1849, 0.1776, 0.2038,\n",
       "        0.1773, 0.1878, 0.1793, 0.1960, 0.1935, 0.1786, 0.1532, 0.1185, 0.2015,\n",
       "        0.1907, 0.2112, 0.1967, 0.2037, 0.1994, 0.0528, 0.1832, 0.1633, 0.1812,\n",
       "        0.1988, 0.1742, 0.2177, 0.1901, 0.1778, 0.0706, 0.1987, 0.2417, 0.1658,\n",
       "        0.1840, 0.1763, 0.1950, 0.2085, 0.1906, 0.2025, 0.1713, 0.2475, 0.1939,\n",
       "        0.1755, 0.1929, 0.1378, 0.1944, 0.1803, 0.1839, 0.1617, 0.1919, 0.1710,\n",
       "        0.1861, 0.1589, 0.2092, 0.2252, 0.1949, 0.2080, 0.1775, 0.1984, 0.1842,\n",
       "        0.1919, 0.2261, 0.1953, 0.1940, 0.2496, 0.2153, 0.0501, 0.1797, 0.2050,\n",
       "        0.2279, 0.1993, 0.2056, 0.2081, 0.1783, 0.1789, 0.1948, 0.1909, 0.1657,\n",
       "        0.1894, 0.1901, 0.1939, 0.1997, 0.1939, 0.1790, 0.2071, 0.1217, 0.1811,\n",
       "        0.1743, 0.2202, 0.1910, 0.1884, 0.2246, 0.1929, 0.2057, 0.1751, 0.1968,\n",
       "        0.1799, 0.1807, 0.1856, 0.1968, 0.2126, 0.1744, 0.2287, 0.1813, 0.2130,\n",
       "        0.2163, 0.2274, 0.1855, 0.1663, 0.1714, 0.1806, 0.1870, 0.2162, 0.1918,\n",
       "        0.1621, 0.1802, 0.2120, 0.1645, 0.2075, 0.1612, 0.1757, 0.1968, 0.2067,\n",
       "        0.0430, 0.0715, 0.1992, 0.1713, 0.2130, 0.2078, 0.0492, 0.1846, 0.2049,\n",
       "        0.1995, 0.1914, 0.1975, 0.1758, 0.2057, 0.1663, 0.2204, 0.2045, 0.1877,\n",
       "        0.0608, 0.0551, 0.2212, 0.1949, 0.1891, 0.2025, 0.1979, 0.1851, 0.1910,\n",
       "        0.1713, 0.1884, 0.1987, 0.0643, 0.1914, 0.2395, 0.1831, 0.1902, 0.1741,\n",
       "        0.1919, 0.1812, 0.0681, 0.2024, 0.1959, 0.0526, 0.1893, 0.2065, 0.0969,\n",
       "        0.1988, 0.1940, 0.1956, 0.2085, 0.2012, 0.0678, 0.1812, 0.1821, 0.1736,\n",
       "        0.1892, 0.1933, 0.0839, 0.1738, 0.2093, 0.1908, 0.1714, 0.1975, 0.2014,\n",
       "        0.1891, 0.1953, 0.2019, 0.2165, 0.1870, 0.1935, 0.2164, 0.1846, 0.1841,\n",
       "        0.1762, 0.2349, 0.1905, 0.1667, 0.1910, 0.2093, 0.1944, 0.2072, 0.2027,\n",
       "        0.0504, 0.1939, 0.2013, 0.1845, 0.1919, 0.0686, 0.1734, 0.1742, 0.1937,\n",
       "        0.2194, 0.0626, 0.0836, 0.1880, 0.1772, 0.0583, 0.2104, 0.1748, 0.1763,\n",
       "        0.1865, 0.2027, 0.1951, 0.2061, 0.1503, 0.0895, 0.1831, 0.1987, 0.0482,\n",
       "        0.1990, 0.2252, 0.2008, 0.1842, 0.1812, 0.2108, 0.2153, 0.1854, 0.2347,\n",
       "        0.1963, 0.2036, 0.1302, 0.2048, 0.2017, 0.2270, 0.1640, 0.1787, 0.1707,\n",
       "        0.2168, 0.1831, 0.1928, 0.1789, 0.1783, 0.1881, 0.0647, 0.1870, 0.1860,\n",
       "        0.1690, 0.1924, 0.1874, 0.0577, 0.1904, 0.1991, 0.1979, 0.2137, 0.1969,\n",
       "        0.2312, 0.2329, 0.2039, 0.2342, 0.2162, 0.1860, 0.0571, 0.2367, 0.2002,\n",
       "        0.1645, 0.1950, 0.1864, 0.1786, 0.1941, 0.1652, 0.1923, 0.0941, 0.1935,\n",
       "        0.1858, 0.1917, 0.1904, 0.1812, 0.1970, 0.1902, 0.2242, 0.0453, 0.1761,\n",
       "        0.1957, 0.1196, 0.2123, 0.2282, 0.1851, 0.1870, 0.1732, 0.1953, 0.1897,\n",
       "        0.2083, 0.2125, 0.1858, 0.0535, 0.1648, 0.0619, 0.1551, 0.2008, 0.1811,\n",
       "        0.0545, 0.2079, 0.2315, 0.1818, 0.2017, 0.2527, 0.2056, 0.1843, 0.1974,\n",
       "        0.1881, 0.1583, 0.1754, 0.1782, 0.2075, 0.1854, 0.1876, 0.2021, 0.1741,\n",
       "        0.1988, 0.1639, 0.0706, 0.1697, 0.1536, 0.1837, 0.1958, 0.2207, 0.1851,\n",
       "        0.2083, 0.1908, 0.1790, 0.1866, 0.1981, 0.2217, 0.1850, 0.2018, 0.1804,\n",
       "        0.1811, 0.0897, 0.0504, 0.1903, 0.1849, 0.1886, 0.1822, 0.2249, 0.0515,\n",
       "        0.2133, 0.1970, 0.1880, 0.1645, 0.2115, 0.2060, 0.1685, 0.0671, 0.1452,\n",
       "        0.1879, 0.2007, 0.2288, 0.1855, 0.1356, 0.2140, 0.1950, 0.1842, 0.1831,\n",
       "        0.1929, 0.2049, 0.1987, 0.0560, 0.1442, 0.0772, 0.0702, 0.1894, 0.1527,\n",
       "        0.1880, 0.1961, 0.1884, 0.1899, 0.2223, 0.1764, 0.2137, 0.2381, 0.1812,\n",
       "        0.0716, 0.2010, 0.2064, 0.1348, 0.1861, 0.1878, 0.1995, 0.1821, 0.1721,\n",
       "        0.1723, 0.1858, 0.0688, 0.1868, 0.2088, 0.0535, 0.1821, 0.2078, 0.1963,\n",
       "        0.2006, 0.1939, 0.1900, 0.1911, 0.1919, 0.1931, 0.1833, 0.2168, 0.1037,\n",
       "        0.1767, 0.2095, 0.2026, 0.1883, 0.2183, 0.1596, 0.1794, 0.1921, 0.2233,\n",
       "        0.1810, 0.2124, 0.2177, 0.1778, 0.1906, 0.1173, 0.2197, 0.1997, 0.2035,\n",
       "        0.1987, 0.1990, 0.2253, 0.1719, 0.1909, 0.1948, 0.1862, 0.1891, 0.2097,\n",
       "        0.1701, 0.2036, 0.1947, 0.1861, 0.1945, 0.1988, 0.1749, 0.2077, 0.1736,\n",
       "        0.1737, 0.1986, 0.1911, 0.2105, 0.1889, 0.0738, 0.1929, 0.1940, 0.1841,\n",
       "        0.1855, 0.1835, 0.1813, 0.1406, 0.1530, 0.1979, 0.1714, 0.1960, 0.1860,\n",
       "        0.1949, 0.1453, 0.0617, 0.2033, 0.1796, 0.1870, 0.0911, 0.1966, 0.1989,\n",
       "        0.1957, 0.1977, 0.1685, 0.1876, 0.2109, 0.1345, 0.1739, 0.1812, 0.1926,\n",
       "        0.2075, 0.1283, 0.1852, 0.2235, 0.1659, 0.1813, 0.1904, 0.1695, 0.2283,\n",
       "        0.1757, 0.1257, 0.1890, 0.2510, 0.2075, 0.2020, 0.1907, 0.0488, 0.1909,\n",
       "        0.2222, 0.1852, 0.1104, 0.1842, 0.1834, 0.1956, 0.2032, 0.1930, 0.1589,\n",
       "        0.1968, 0.1738, 0.1488, 0.1451, 0.0612, 0.1753, 0.1900, 0.2045, 0.0680,\n",
       "        0.1960, 0.1844, 0.1951, 0.1602, 0.0764, 0.1589, 0.1931, 0.1980, 0.1960,\n",
       "        0.1934, 0.2310, 0.1961, 0.1950, 0.1968, 0.1938, 0.1951, 0.1818, 0.1880,\n",
       "        0.1713, 0.1785, 0.1962, 0.1850, 0.1964, 0.2008, 0.0666, 0.1917, 0.1670,\n",
       "        0.2063, 0.1179, 0.1951, 0.1983, 0.1292, 0.1857, 0.1833, 0.0886, 0.2428,\n",
       "        0.1872, 0.2067, 0.1996, 0.1881, 0.1901, 0.1885, 0.1984, 0.0754, 0.2066,\n",
       "        0.0607, 0.0754, 0.2060, 0.2116, 0.0556, 0.1792, 0.1748, 0.1841, 0.1743,\n",
       "        0.1662, 0.1982, 0.1582, 0.1935, 0.2182, 0.2067, 0.1855, 0.1778, 0.1900,\n",
       "        0.2124, 0.1215, 0.2092, 0.1929, 0.2434, 0.1936, 0.1948, 0.0622, 0.1852,\n",
       "        0.1868, 0.2035, 0.2310, 0.1794, 0.1655, 0.1756, 0.2074, 0.2194, 0.2152,\n",
       "        0.0502, 0.2294, 0.1950, 0.2149, 0.2024, 0.1727, 0.0657, 0.1919, 0.1847,\n",
       "        0.1900, 0.1825, 0.1898], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].ln_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4bd9d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0019, -0.1037,  0.0011,  ...,  0.1639, -0.0937,  0.0341],\n",
       "         [ 0.1208, -0.0494, -0.0716,  ..., -0.0105, -0.1055, -0.0614],\n",
       "         [ 0.0803, -0.0301,  0.0376,  ..., -0.0025, -0.1169, -0.1349],\n",
       "         ...,\n",
       "         [ 0.0287,  0.1587,  0.0989,  ..., -0.2045,  0.0797, -0.0762],\n",
       "         [-0.1655, -0.1087,  0.0521,  ..., -0.1481, -0.1840,  0.0692],\n",
       "         [ 0.1190, -0.0457,  0.0814,  ..., -0.0793, -0.2008,  0.1165]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.transformer.h[0].ln_1(hs[0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e454532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,k,v = model.transformer.h[0].attn.c_attn(a).split((768, 768, 768), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f69f6749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e2427a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0][:, :64].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ec7c1240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0912, -0.3976, -0.1741,  ..., -0.6445,  0.8169,  0.0283],\n",
       "         [ 0.2345,  0.4826,  0.0843,  ..., -0.7618,  1.3198,  0.8108],\n",
       "         [ 1.0434,  0.2520, -0.3885,  ..., -0.3062, -0.7437,  0.4505],\n",
       "         ...,\n",
       "         [-0.5736, -0.9146, -1.0684,  ..., -0.4004,  1.0381, -1.4904],\n",
       "         [ 1.3110, -1.4384, -0.5634,  ..., -0.7802, -0.4422,  0.5120],\n",
       "         [-0.0185, -0.7402, -0.2828,  ..., -0.7718,  0.6120,  0.4303]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " tensor([[-1.0132, -2.4647, -1.9612,  ..., -3.2596, -3.1811, -2.5158],\n",
       "         [ 2.1841,  2.4373,  2.0603,  ...,  2.4532,  2.7200,  1.9560],\n",
       "         [ 0.5509,  2.5695,  1.9895,  ...,  1.4560,  2.0114,  3.3688],\n",
       "         ...,\n",
       "         [-1.4600, -1.3429, -1.5031,  ..., -1.4433, -0.7054, -2.1890],\n",
       "         [-0.3480, -1.6592, -2.1551,  ..., -3.2121, -2.0480, -1.0909],\n",
       "         [ 1.3589,  1.9584,  1.9182,  ...,  1.0802,  1.3847,  3.0028]],\n",
       "        grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q[0][:, :64], k[0][:, :64].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "50512daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.4854,  -8.8071, -11.4450, -13.1997, -10.8294, -11.9119, -13.0449,\n",
       "          -4.3931, -11.3048, -11.4803, -13.5225,  -9.5102, -15.5763, -12.9939,\n",
       "         -11.9211, -15.6077],\n",
       "        [  9.3668, -13.5898, -23.5171, -20.7397, -22.4040, -20.5137,  -3.0508,\n",
       "         -14.9600, -21.2427, -12.9855, -20.3471, -13.3441, -25.0722, -16.9933,\n",
       "         -21.9441, -12.1705],\n",
       "        [  0.2705, -12.4505, -19.0622, -16.8973, -12.4341, -16.7591, -11.0500,\n",
       "          -8.3409, -15.6627, -10.8733, -14.9455,  -6.5480, -18.0123, -10.3377,\n",
       "         -20.1341, -12.8820],\n",
       "        [  2.3355,  -5.8269, -10.3535, -16.7616,  -9.0833, -17.6115,  -7.2563,\n",
       "         -12.5624, -12.1397,  -8.2168,  -5.8472,  -7.2905, -10.4731,  -8.7743,\n",
       "         -12.0062,  -3.2087],\n",
       "        [ -0.3024,  -6.1341, -11.9915, -10.1298, -16.1768, -11.7815,  -6.2491,\n",
       "           2.1572,  -9.8635,  -4.6895,  -8.6208,  -0.4197,  -8.4663,   0.2582,\n",
       "          -9.2789,  -9.8129],\n",
       "        [ -1.9413, -13.1588, -22.6552, -22.5430, -19.7533, -33.3197, -12.1519,\n",
       "          -8.4387, -26.1073, -11.4727, -13.4149, -10.4873, -22.6446, -11.6352,\n",
       "         -17.7349,  -9.0250],\n",
       "        [ -3.8704,  -7.4087, -15.2602,  -8.6440, -12.5263, -12.7100, -12.2176,\n",
       "          -4.5656, -15.9766,  -7.9608,  -0.8566,  -1.2500,  -6.5286,   1.1088,\n",
       "           0.3414, -13.7074],\n",
       "        [  0.8540, -11.4324, -11.9761, -14.4967, -15.6558,  -7.8118,  -8.2252,\n",
       "         -11.2735,  -3.1967, -11.2313, -10.5064,  -6.5334, -11.7344, -10.3923,\n",
       "          -2.3333,  -5.0379],\n",
       "        [ -1.4187,  -9.0948, -17.6271, -17.5711, -16.7415, -26.8476,  -8.1042,\n",
       "          -9.8975, -26.7997,  -9.8981,  -5.6476,  -9.0482, -14.1520,  -3.9591,\n",
       "          -2.2907,  -1.9736],\n",
       "        [  0.8797,  -7.2998, -10.5054, -14.6050, -10.7980, -17.4909,  -4.1654,\n",
       "         -11.0964, -10.9005, -10.1966,  -5.1941, -16.9228,  -8.9502, -10.2504,\n",
       "         -16.9974,  -1.7677],\n",
       "        [ -0.1599, -13.3919, -21.4608, -16.9986, -14.5852, -19.0542,  -8.7535,\n",
       "          -9.7434, -16.0687,  -7.1367, -19.3172,  -6.8556, -20.2317, -13.1269,\n",
       "         -18.3114,  -7.9982],\n",
       "        [  9.0768,  -2.5756,  -7.5912, -14.0698,  -8.5373,  -9.2277,  -8.8807,\n",
       "          -6.4614,  -2.1320,  -8.2664,  -2.9355,   2.4272,  -9.5091,  -6.9423,\n",
       "          -2.4503, -12.6638],\n",
       "        [ -4.9914, -16.3438, -22.2277, -21.9359, -17.3748, -28.1633, -16.5890,\n",
       "         -12.1212, -21.9619, -11.4775, -18.3798, -11.6027, -24.1828, -14.6825,\n",
       "         -23.8334,  -7.4919],\n",
       "        [ -2.2303, -19.5714, -12.2445, -23.3912, -12.5469, -20.3380, -12.0486,\n",
       "         -17.5523, -15.9448, -15.9723, -18.8065,  -7.7161, -15.6873, -15.2522,\n",
       "         -14.2728, -17.8202],\n",
       "        [ -4.9569, -15.4445, -14.5843, -17.2695, -14.3267, -18.0885,  -6.1416,\n",
       "          -9.7499, -14.9531,  -8.7201, -13.9171, -11.8290, -17.5917,  -8.3399,\n",
       "         -19.3132,  -7.0074],\n",
       "        [ -0.4348,  -6.8676, -15.9050, -10.1480,  -6.1895,  -8.6180, -11.2236,\n",
       "          -2.0799,  -3.6983,  -6.1658,  -8.0782,  -8.2293, -12.2870, -13.3590,\n",
       "          -9.5850, -11.0513]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q[0][:, :64] @ k[0][:, :64].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "086edacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "76cb741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gen\n",
    "import torch\n",
    "importlib.reload(gen)\n",
    "\n",
    "g = gen.Graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    g.add_tensor(k, v, is_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eab9814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = g.add_tensor(\"toks\", torch.tensor([1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ed13ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class lodash:\n",
    "    def __init__(self):\n",
    "        self.g = gen.Graph()\n",
    "        pass\n",
    "\n",
    "    def t(self, name: str, t: torch.Tensor, is_weight=False):\n",
    "        return self.g.add_tensor(name, t, is_weight)\n",
    "\n",
    "    def op(self, name: str, num_blocks: int, inps: list[int], outs: list[int], consts: list[int] = []):\n",
    "        self.g.add_kernel_op(name, num_blocks, inps, outs, consts)\n",
    "    \n",
    "    def __getitem__(self, name: str):\n",
    "        return self.g.get_tensor_idx(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746c85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dd1b1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = g.add_tensor(\"x\", torch.rand((4, 768)))\n",
    "g.add_kernel_op(\"embedding\", 10, [toks, g.get_tensor_idx(\"transformer.wte.weight\")], [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b2a10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    embedding((toks),(transformer_wte_weight),(x), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "print(g.emit([toks], [x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d6fdbd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                typedef struct {\n",
      "                    float* transformer.wte.weight;\n",
      "float* transformer.wpe.weight;\n",
      "float* transformer.h.0.ln_1.weight;\n",
      "float* transformer.h.0.ln_1.bias;\n",
      "float* transformer.h.0.attn.c_attn.weight;\n",
      "float* transformer.h.0.attn.c_attn.bias;\n",
      "float* transformer.h.0.attn.c_proj.weight;\n",
      "float* transformer.h.0.attn.c_proj.bias;\n",
      "float* transformer.h.0.ln_2.weight;\n",
      "float* transformer.h.0.ln_2.bias;\n",
      "float* transformer.h.0.mlp.c_fc.weight;\n",
      "float* transformer.h.0.mlp.c_fc.bias;\n",
      "float* transformer.h.0.mlp.c_proj.weight;\n",
      "float* transformer.h.0.mlp.c_proj.bias;\n",
      "float* transformer.h.1.ln_1.weight;\n",
      "float* transformer.h.1.ln_1.bias;\n",
      "float* transformer.h.1.attn.c_attn.weight;\n",
      "float* transformer.h.1.attn.c_attn.bias;\n",
      "float* transformer.h.1.attn.c_proj.weight;\n",
      "float* transformer.h.1.attn.c_proj.bias;\n",
      "float* transformer.h.1.ln_2.weight;\n",
      "float* transformer.h.1.ln_2.bias;\n",
      "float* transformer.h.1.mlp.c_fc.weight;\n",
      "float* transformer.h.1.mlp.c_fc.bias;\n",
      "float* transformer.h.1.mlp.c_proj.weight;\n",
      "float* transformer.h.1.mlp.c_proj.bias;\n",
      "float* transformer.h.2.ln_1.weight;\n",
      "float* transformer.h.2.ln_1.bias;\n",
      "float* transformer.h.2.attn.c_attn.weight;\n",
      "float* transformer.h.2.attn.c_attn.bias;\n",
      "float* transformer.h.2.attn.c_proj.weight;\n",
      "float* transformer.h.2.attn.c_proj.bias;\n",
      "float* transformer.h.2.ln_2.weight;\n",
      "float* transformer.h.2.ln_2.bias;\n",
      "float* transformer.h.2.mlp.c_fc.weight;\n",
      "float* transformer.h.2.mlp.c_fc.bias;\n",
      "float* transformer.h.2.mlp.c_proj.weight;\n",
      "float* transformer.h.2.mlp.c_proj.bias;\n",
      "float* transformer.h.3.ln_1.weight;\n",
      "float* transformer.h.3.ln_1.bias;\n",
      "float* transformer.h.3.attn.c_attn.weight;\n",
      "float* transformer.h.3.attn.c_attn.bias;\n",
      "float* transformer.h.3.attn.c_proj.weight;\n",
      "float* transformer.h.3.attn.c_proj.bias;\n",
      "float* transformer.h.3.ln_2.weight;\n",
      "float* transformer.h.3.ln_2.bias;\n",
      "float* transformer.h.3.mlp.c_fc.weight;\n",
      "float* transformer.h.3.mlp.c_fc.bias;\n",
      "float* transformer.h.3.mlp.c_proj.weight;\n",
      "float* transformer.h.3.mlp.c_proj.bias;\n",
      "float* transformer.h.4.ln_1.weight;\n",
      "float* transformer.h.4.ln_1.bias;\n",
      "float* transformer.h.4.attn.c_attn.weight;\n",
      "float* transformer.h.4.attn.c_attn.bias;\n",
      "float* transformer.h.4.attn.c_proj.weight;\n",
      "float* transformer.h.4.attn.c_proj.bias;\n",
      "float* transformer.h.4.ln_2.weight;\n",
      "float* transformer.h.4.ln_2.bias;\n",
      "float* transformer.h.4.mlp.c_fc.weight;\n",
      "float* transformer.h.4.mlp.c_fc.bias;\n",
      "float* transformer.h.4.mlp.c_proj.weight;\n",
      "float* transformer.h.4.mlp.c_proj.bias;\n",
      "float* transformer.h.5.ln_1.weight;\n",
      "float* transformer.h.5.ln_1.bias;\n",
      "float* transformer.h.5.attn.c_attn.weight;\n",
      "float* transformer.h.5.attn.c_attn.bias;\n",
      "float* transformer.h.5.attn.c_proj.weight;\n",
      "float* transformer.h.5.attn.c_proj.bias;\n",
      "float* transformer.h.5.ln_2.weight;\n",
      "float* transformer.h.5.ln_2.bias;\n",
      "float* transformer.h.5.mlp.c_fc.weight;\n",
      "float* transformer.h.5.mlp.c_fc.bias;\n",
      "float* transformer.h.5.mlp.c_proj.weight;\n",
      "float* transformer.h.5.mlp.c_proj.bias;\n",
      "float* transformer.h.6.ln_1.weight;\n",
      "float* transformer.h.6.ln_1.bias;\n",
      "float* transformer.h.6.attn.c_attn.weight;\n",
      "float* transformer.h.6.attn.c_attn.bias;\n",
      "float* transformer.h.6.attn.c_proj.weight;\n",
      "float* transformer.h.6.attn.c_proj.bias;\n",
      "float* transformer.h.6.ln_2.weight;\n",
      "float* transformer.h.6.ln_2.bias;\n",
      "float* transformer.h.6.mlp.c_fc.weight;\n",
      "float* transformer.h.6.mlp.c_fc.bias;\n",
      "float* transformer.h.6.mlp.c_proj.weight;\n",
      "float* transformer.h.6.mlp.c_proj.bias;\n",
      "float* transformer.h.7.ln_1.weight;\n",
      "float* transformer.h.7.ln_1.bias;\n",
      "float* transformer.h.7.attn.c_attn.weight;\n",
      "float* transformer.h.7.attn.c_attn.bias;\n",
      "float* transformer.h.7.attn.c_proj.weight;\n",
      "float* transformer.h.7.attn.c_proj.bias;\n",
      "float* transformer.h.7.ln_2.weight;\n",
      "float* transformer.h.7.ln_2.bias;\n",
      "float* transformer.h.7.mlp.c_fc.weight;\n",
      "float* transformer.h.7.mlp.c_fc.bias;\n",
      "float* transformer.h.7.mlp.c_proj.weight;\n",
      "float* transformer.h.7.mlp.c_proj.bias;\n",
      "float* transformer.h.8.ln_1.weight;\n",
      "float* transformer.h.8.ln_1.bias;\n",
      "float* transformer.h.8.attn.c_attn.weight;\n",
      "float* transformer.h.8.attn.c_attn.bias;\n",
      "float* transformer.h.8.attn.c_proj.weight;\n",
      "float* transformer.h.8.attn.c_proj.bias;\n",
      "float* transformer.h.8.ln_2.weight;\n",
      "float* transformer.h.8.ln_2.bias;\n",
      "float* transformer.h.8.mlp.c_fc.weight;\n",
      "float* transformer.h.8.mlp.c_fc.bias;\n",
      "float* transformer.h.8.mlp.c_proj.weight;\n",
      "float* transformer.h.8.mlp.c_proj.bias;\n",
      "float* transformer.h.9.ln_1.weight;\n",
      "float* transformer.h.9.ln_1.bias;\n",
      "float* transformer.h.9.attn.c_attn.weight;\n",
      "float* transformer.h.9.attn.c_attn.bias;\n",
      "float* transformer.h.9.attn.c_proj.weight;\n",
      "float* transformer.h.9.attn.c_proj.bias;\n",
      "float* transformer.h.9.ln_2.weight;\n",
      "float* transformer.h.9.ln_2.bias;\n",
      "float* transformer.h.9.mlp.c_fc.weight;\n",
      "float* transformer.h.9.mlp.c_fc.bias;\n",
      "float* transformer.h.9.mlp.c_proj.weight;\n",
      "float* transformer.h.9.mlp.c_proj.bias;\n",
      "float* transformer.h.10.ln_1.weight;\n",
      "float* transformer.h.10.ln_1.bias;\n",
      "float* transformer.h.10.attn.c_attn.weight;\n",
      "float* transformer.h.10.attn.c_attn.bias;\n",
      "float* transformer.h.10.attn.c_proj.weight;\n",
      "float* transformer.h.10.attn.c_proj.bias;\n",
      "float* transformer.h.10.ln_2.weight;\n",
      "float* transformer.h.10.ln_2.bias;\n",
      "float* transformer.h.10.mlp.c_fc.weight;\n",
      "float* transformer.h.10.mlp.c_fc.bias;\n",
      "float* transformer.h.10.mlp.c_proj.weight;\n",
      "float* transformer.h.10.mlp.c_proj.bias;\n",
      "float* transformer.h.11.ln_1.weight;\n",
      "float* transformer.h.11.ln_1.bias;\n",
      "float* transformer.h.11.attn.c_attn.weight;\n",
      "float* transformer.h.11.attn.c_attn.bias;\n",
      "float* transformer.h.11.attn.c_proj.weight;\n",
      "float* transformer.h.11.attn.c_proj.bias;\n",
      "float* transformer.h.11.ln_2.weight;\n",
      "float* transformer.h.11.ln_2.bias;\n",
      "float* transformer.h.11.mlp.c_fc.weight;\n",
      "float* transformer.h.11.mlp.c_fc.bias;\n",
      "float* transformer.h.11.mlp.c_proj.weight;\n",
      "float* transformer.h.11.mlp.c_proj.bias;\n",
      "float* transformer.ln_f.weight;\n",
      "float* transformer.ln_f.bias;\n",
      "float* lm_head.weight;\n",
      "                } W;\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "print(g.emit_weight_struct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.emit_weight_allocator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = g.add_tensor(\"x\", torch.rand((4, 768)))\n",
    "g.add_kernel_op(\"embedding\", 10, [toks, g.get_tensor_idx(\"transformer.wte.weight\")], [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ab88c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lodash()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f71354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EMBD = 768\n",
    "N_HEAD = 12\n",
    "N_LAYER = 12\n",
    "VOCAB_SIZE = 50257\n",
    "HEAD_DIM = N_EMBD // N_HEAD  # 64\n",
    "N_INNER = 4 * N_EMBD        # 3072\n",
    "EPS = 1e-5\n",
    "THREADS = 256\n",
    "NUM_BLOCKS = 512\n",
    "BATCH = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3abe7938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbea971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/herocharge/Library/Python/3.9/lib/python/site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "# install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e18ea7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lodash()\n",
    "\n",
    "# add all weights\n",
    "for k, v in model.state_dict().items():\n",
    "    _.t(k, v, is_weight=True)\n",
    "\n",
    "SEQ_LEN = 100\n",
    "\n",
    "toks = _.t(\"toks\", torch.tensor([[1, 2, 3, 4]]*BATCH))\n",
    "\n",
    "embed_out = _.t(\"embed_out\", torch.rand((BATCH, SEQ_LEN, N_EMBD)))\n",
    "_.op(\"embedding\", 10, [toks, _[\"transformer.wte.weight\"]], [embed_out])\n",
    "\n",
    "\n",
    "pos_out = _.t(\"pos_out\", torch.rand((BATCH, SEQ_LEN, N_EMBD)))\n",
    "_.op(\"position\", 10, [embed_out, _[\"transformer.wpe.weight\"]], [pos_out])\n",
    "\n",
    "res = _.t(\"residual\", torch.rand((BATCH, SEQ_LEN, N_EMBD)))\n",
    "x = pos_out\n",
    "\n",
    "for layer_num in range(N_LAYER):\n",
    "    \n",
    "    ln1_out = _.t(f\"ln1_out_{layer_num}\", torch.rand((BATCH, SEQ_LEN, N_EMBD)))\n",
    "    _.op(\"layer_norm\", 10, [x, _[f\"transformer.h.{layer_num}.ln_1.weight\"], _[f\"transformer.h.{layer_num}.ln_1.bias\"]], [ln1_out])\n",
    "    \n",
    "    qkv = _.t(f\"qkv_{layer_num}\", torch.rand((BATCH, SEQ_LEN, 3, N_EMBD)))\n",
    "    _.op(f\"linear_layer\", 10, [ln1_out, _[f\"transformer.h.{layer_num}.attn.c_attn.weight\"], _[f\"transformer.h.{layer_num}.attn.c_attn.bias\"]], [qkv])\n",
    "    \n",
    "    attn_scores = _.t(f\"a_scores_{layer_num}\", torch.rand((BATCH, N_HEAD, SEQ_LEN, SEQ_LEN)))\n",
    "    _.op(\"attn\", 10, [qkv], [attn_scores])\n",
    "\n",
    "    # TODO: masking\n",
    "\n",
    "    attn_probs = _.t(f\"a_probs_{layer_num}\", torch.rand((BATCH, N_HEAD, SEQ_LEN, SEQ_LEN)))\n",
    "    _.op(\"softmax\", 10, [attn_scores], [attn_probs])\n",
    "\n",
    "    y = _.t(f\"y_{layer_num}\", torch.rand((BATCH, SEQ_LEN, N_EMBD)))\n",
    "    _.op(\"v_mul\", 10, [attn_probs, qkv], [y])\n",
    "\n",
    "\n",
    "    x = y\n",
    "    # TODO: dropouo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "92951a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    embedding((toks),(transformer_wte_weight),(embed_out), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    position((embed_out),(transformer_wpe_weight),(pos_out), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((pos_out),(transformer_h_0_ln_1_weight),(transformer_h_0_ln_1_bias),(ln1_out_0), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_0),(transformer_h_0_attn_c_attn_weight),(transformer_h_0_attn_c_attn_bias),(qkv_0), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_0),(a_scores_0), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_0),(a_probs_0), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_0),(qkv_0),(y_0), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_0),(transformer_h_1_ln_1_weight),(transformer_h_1_ln_1_bias),(ln1_out_1), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_1),(transformer_h_1_attn_c_attn_weight),(transformer_h_1_attn_c_attn_bias),(qkv_1), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_1),(a_scores_1), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_1),(a_probs_1), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_1),(qkv_1),(y_1), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_1),(transformer_h_2_ln_1_weight),(transformer_h_2_ln_1_bias),(ln1_out_2), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_2),(transformer_h_2_attn_c_attn_weight),(transformer_h_2_attn_c_attn_bias),(qkv_2), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_2),(a_scores_2), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_2),(a_probs_2), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_2),(qkv_2),(y_2), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_2),(transformer_h_3_ln_1_weight),(transformer_h_3_ln_1_bias),(ln1_out_3), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_3),(transformer_h_3_attn_c_attn_weight),(transformer_h_3_attn_c_attn_bias),(qkv_3), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_3),(a_scores_3), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_3),(a_probs_3), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_3),(qkv_3),(y_3), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_3),(transformer_h_4_ln_1_weight),(transformer_h_4_ln_1_bias),(ln1_out_4), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_4),(transformer_h_4_attn_c_attn_weight),(transformer_h_4_attn_c_attn_bias),(qkv_4), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_4),(a_scores_4), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_4),(a_probs_4), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_4),(qkv_4),(y_4), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_4),(transformer_h_5_ln_1_weight),(transformer_h_5_ln_1_bias),(ln1_out_5), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_5),(transformer_h_5_attn_c_attn_weight),(transformer_h_5_attn_c_attn_bias),(qkv_5), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_5),(a_scores_5), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_5),(a_probs_5), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_5),(qkv_5),(y_5), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_5),(transformer_h_6_ln_1_weight),(transformer_h_6_ln_1_bias),(ln1_out_6), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_6),(transformer_h_6_attn_c_attn_weight),(transformer_h_6_attn_c_attn_bias),(qkv_6), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_6),(a_scores_6), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_6),(a_probs_6), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_6),(qkv_6),(y_6), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_6),(transformer_h_7_ln_1_weight),(transformer_h_7_ln_1_bias),(ln1_out_7), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_7),(transformer_h_7_attn_c_attn_weight),(transformer_h_7_attn_c_attn_bias),(qkv_7), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_7),(a_scores_7), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_7),(a_probs_7), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_7),(qkv_7),(y_7), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_7),(transformer_h_8_ln_1_weight),(transformer_h_8_ln_1_bias),(ln1_out_8), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_8),(transformer_h_8_attn_c_attn_weight),(transformer_h_8_attn_c_attn_bias),(qkv_8), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_8),(a_scores_8), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_8),(a_probs_8), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_8),(qkv_8),(y_8), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_8),(transformer_h_9_ln_1_weight),(transformer_h_9_ln_1_bias),(ln1_out_9), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_9),(transformer_h_9_attn_c_attn_weight),(transformer_h_9_attn_c_attn_bias),(qkv_9), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_9),(a_scores_9), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_9),(a_probs_9), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_9),(qkv_9),(y_9), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_9),(transformer_h_10_ln_1_weight),(transformer_h_10_ln_1_bias),(ln1_out_10), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_10),(transformer_h_10_attn_c_attn_weight),(transformer_h_10_attn_c_attn_bias),(qkv_10), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_10),(a_scores_10), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_10),(a_probs_10), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_10),(qkv_10),(y_10), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    layer_norm((y_10),(transformer_h_11_ln_1_weight),(transformer_h_11_ln_1_bias),(ln1_out_11), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    linear_layer((ln1_out_11),(transformer_h_11_attn_c_attn_weight),(transformer_h_11_attn_c_attn_bias),(qkv_11), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    attn((qkv_11),(a_scores_11), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    softmax((a_scores_11),(a_probs_11), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n",
      "                    \n",
      "                for(int bidx = blockIdx.x; bidx < 10; bidx += blockDim.x) {\n",
      "                    v_mul((a_probs_11),(qkv_11),(y_11), bidx);\n",
      "                }\n",
      "            \n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "print(_.g.emit(start_idx=[toks], end_idx=[_[\"y_11\"]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
